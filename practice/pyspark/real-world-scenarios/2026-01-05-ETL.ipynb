{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c772e4-d78e-4b3f-b5fe-7d954711d431",
   "metadata": {},
   "source": [
    "### PySpark Data Processing Project Checklist\n",
    "\n",
    "#### 1. Data Loading\n",
    "- [ ] Load `orders.csv` into a Spark DataFrame\n",
    "- [ ] Load `customers.csv` into a Spark DataFrame\n",
    "\n",
    "#### 2. Data Transformation\n",
    "- [ ] Merge the datasets on `customer_id`\n",
    "- [ ] Make all columns uppercase (optional)\n",
    "- [ ] Calculate each customer's total spend across all orders\n",
    "- [ ] Create a new column `days_since_first_order` indicating the number of days since the customer first ordered\n",
    "\n",
    "#### 3. Aggregation\n",
    "- [ ] Group data by `product_category`\n",
    "- [ ] Calculate the total order amount for each category\n",
    "- [ ] Identify the top 3 customers based on total spend\n",
    "\n",
    "#### 4. Data Export\n",
    "- [ ] Create `customer_spend.csv` with columns:\n",
    "  - [ ] `customer_id`\n",
    "  - [ ] `customer_name`\n",
    "  - [ ] `total_spend`\n",
    "  - [ ] `days_since_first_order`\n",
    "- [ ] Create `category_summary.csv` with columns:\n",
    "  - [ ] `product_category`\n",
    "  - [ ] `total_order_amount`\n",
    "- [ ] Verify both CSV files were saved correctly\n",
    "\n",
    "#### 5. Validation (Optional)\n",
    "- [ ] Verify row counts match expectations\n",
    "- [ ] Check for null values in critical columns\n",
    "- [ ] Validate calculations with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4318c64c-2ab0-4875-9ab6-df869d1a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session \n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sum, avg, count, row_number, round, dayofmonth, min, max, current_date, datediff, upper, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"ecomm_orders_etl\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82108c5e-8653-4e0e-be03-eba5f622e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+----------------+\n",
      "|order_id|customer_id|order_date|order_amount|product_category|\n",
      "+--------+-----------+----------+------------+----------------+\n",
      "|       1|        101|2025-01-10|         250|     Electronics|\n",
      "|       2|        102|2025-02-15|         180|         Fashion|\n",
      "|       3|        103|2025-02-17|          70|       Groceries|\n",
      "|       4|        101|2025-03-01|         300|     Electronics|\n",
      "|       5|        104|2025-03-12|         120|          Health|\n",
      "+--------+-----------+----------+------------+----------------+\n",
      "\n",
      "+-----------+-------------+--------------------+--------------+\n",
      "|customer_id|customer_name|      customer_email|customer_since|\n",
      "+-----------+-------------+--------------------+--------------+\n",
      "|        101|     John Doe| johndoe@example.com|    2024-07-10|\n",
      "|        102|   Jane Smith|janesmith@example...|    2023-12-05|\n",
      "|        103|  Emily Davis|emilydavis@exampl...|    2025-01-20|\n",
      "|        104|Michael Brown|michaelbrown@exam...|    2024-09-15|\n",
      "+-----------+-------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "orders_df = spark.read.csv(\"source_data/orders.csv\", header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(\"source_data/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#validate \n",
    "orders_df.show(5)\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3295b253-ae62-48df-a596-9aae54fff4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+----------------+-------------+--------------------+--------------+-----------+----------------+----------------------+\n",
      "|customer_id|order_id|order_date|order_amount|product_category|customer_name|      customer_email|customer_since|total_spend|first_order_date|days_since_first_order|\n",
      "+-----------+--------+----------+------------+----------------+-------------+--------------------+--------------+-----------+----------------+----------------------+\n",
      "|        101|       4|2025-03-01|         300|     Electronics|     John Doe| johndoe@example.com|    2024-07-10|        550|      2025-01-10|                   310|\n",
      "|        101|       1|2025-01-10|         250|     Electronics|     John Doe| johndoe@example.com|    2024-07-10|        550|      2025-01-10|                   360|\n",
      "|        102|       2|2025-02-15|         180|         Fashion|   Jane Smith|janesmith@example...|    2023-12-05|        180|      2025-02-15|                   324|\n",
      "|        103|       3|2025-02-17|          70|       Groceries|  Emily Davis|emilydavis@exampl...|    2025-01-20|         70|      2025-02-17|                   322|\n",
      "|        104|       5|2025-03-12|         120|          Health|Michael Brown|michaelbrown@exam...|    2024-09-15|        120|      2025-03-12|                   299|\n",
      "+-----------+--------+----------+------------+----------------+-------------+--------------------+--------------+-----------+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformations \n",
    "# 2.1 Merge the datasets on customer_id\n",
    "joined_df = orders_df.join(customers_df, \"customer_id\")\n",
    "\n",
    "# 2.2 Make all columns uppercase (optional)\n",
    "\n",
    "# 2.3 Calculate each customer's total spend across all orders\n",
    "# 2.4 Create a new column days_since_first_order indicating the number of days since the customer first ordered\n",
    "window_spec = Window.partitionBy(\"customer_id\")\n",
    "t2DF = joined_df.withColumn(\"total_spend\", sum(col(\"order_amount\")).over(window_spec)) \\\n",
    "            .withColumn(\"first_order_date\", min(col(\"order_date\")).over(window_spec)) \\\n",
    "    .withColumn(\"days_since_first_order\", datediff(current_date(), (col(\"order_date\"))))\n",
    "\n",
    "t2DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f36d310-7b97-4db5-a406-c3cc4ef250a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------------+------------+\n",
      "|product_category|customer_id|total_order_amount|ranked_spend|\n",
      "+----------------+-----------+------------------+------------+\n",
      "|     Electronics|        101|               550|           1|\n",
      "|         Fashion|        102|               180|           1|\n",
      "|       Groceries|        103|                70|           1|\n",
      "|          Health|        104|               120|           1|\n",
      "+----------------+-----------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggreation \n",
    "# 3.1 Group data by product_category\n",
    "# 3.2 Calculate the total order amount for each category\n",
    "# 3.3 Identify the top 3 customers based on total spend\n",
    "from pyspark.sql.functions import dense_rank\n",
    "t3DF = t2DF.groupBy(\"product_category\", \"customer_id\").agg(sum(\"order_amount\").alias(\"total_order_amount\"))\n",
    "\n",
    "window_spec_ = Window.partitionBy(\"customer_id\").orderBy(col(\"total_order_amount\").desc())\n",
    "t4DF = t3DF.withColumn(\"ranked_spend\", dense_rank().over(window_spec_)) \\\n",
    "            .filter(col(\"ranked_spend\") <= 3)\n",
    "t4DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7259599-4a09-4230-824b-1ba014537b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data export \n",
    "customer_spendDF = t2DF.select(\"customer_id\", \"customer_name\", \"total_spend\", \"days_since_first_order\")\n",
    "category_summary_DF = t3DF.select(\"product_category\", \"total_order_amount\" )\n",
    "\n",
    "customer_spendDF.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output_data/customer_spend\")\n",
    "category_summary_DF.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output_data/category_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd7155-24a8-4e15-8732-e04be136bf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning Journal (PySpark)",
   "language": "python",
   "name": "learning-journal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
