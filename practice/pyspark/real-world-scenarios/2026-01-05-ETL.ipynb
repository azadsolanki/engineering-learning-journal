{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c772e4-d78e-4b3f-b5fe-7d954711d431",
   "metadata": {},
   "source": [
    "### PySpark Data Processing Project Checklist\n",
    "\n",
    "#### 1. Data Loading\n",
    "- [ ] Load `orders.csv` into a Spark DataFrame\n",
    "- [ ] Load `customers.csv` into a Spark DataFrame\n",
    "\n",
    "#### 2. Data Transformation\n",
    "- [ ] Merge the datasets on `customer_id`\n",
    "- [ ] Make all columns uppercase (optional)\n",
    "- [ ] Calculate each customer's total spend across all orders\n",
    "- [ ] Create a new column `days_since_first_order` indicating the number of days since the customer first ordered\n",
    "\n",
    "#### 3. Aggregation\n",
    "- [ ] Group data by `product_category`\n",
    "- [ ] Calculate the total order amount for each category\n",
    "- [ ] Identify the top 3 customers based on total spend\n",
    "\n",
    "#### 4. Data Export\n",
    "- [ ] Create `customer_spend.csv` with columns:\n",
    "  - [ ] `customer_id`\n",
    "  - [ ] `customer_name`\n",
    "  - [ ] `total_spend`\n",
    "  - [ ] `days_since_first_order`\n",
    "- [ ] Create `category_summary.csv` with columns:\n",
    "  - [ ] `product_category`\n",
    "  - [ ] `total_order_amount`\n",
    "- [ ] Verify both CSV files were saved correctly\n",
    "\n",
    "#### 5. Validation (Optional)\n",
    "- [ ] Verify row counts match expectations\n",
    "- [ ] Check for null values in critical columns\n",
    "- [ ] Validate calculations with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4318c64c-2ab0-4875-9ab6-df869d1a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session \n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sum, avg, count, row_number, round, dayofmonth, min, max, current_date, datediff, upper, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"ecomm_orders_etl\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82108c5e-8653-4e0e-be03-eba5f622e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+----------------+\n",
      "|order_id|customer_id|order_date|order_amount|product_category|\n",
      "+--------+-----------+----------+------------+----------------+\n",
      "|       1|        101|2025-01-10|         250|     Electronics|\n",
      "|       2|        102|2025-02-15|         180|         Fashion|\n",
      "|       3|        103|2025-02-17|          70|       Groceries|\n",
      "|       4|        101|2025-03-01|         300|     Electronics|\n",
      "|       5|        104|2025-03-12|         120|          Health|\n",
      "+--------+-----------+----------+------------+----------------+\n",
      "\n",
      "+-----------+-------------+--------------------+--------------+\n",
      "|customer_id|customer_name|      customer_email|customer_since|\n",
      "+-----------+-------------+--------------------+--------------+\n",
      "|        101|     John Doe| johndoe@example.com|    2024-07-10|\n",
      "|        102|   Jane Smith|janesmith@example...|    2023-12-05|\n",
      "|        103|  Emily Davis|emilydavis@exampl...|    2025-01-20|\n",
      "|        104|Michael Brown|michaelbrown@exam...|    2024-09-15|\n",
      "+-----------+-------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "orders_df = spark.read.csv(\"source_data/orders.csv\", header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(\"source_data/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#validate \n",
    "orders_df.show(5)\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3295b253-ae62-48df-a596-9aae54fff4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "It is not allowed to use a window function inside an aggregate function. Please use the inner window function in a sub-query.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2.2 Make all columns uppercase (optional)\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2.3 Calculate each customer's total spend across all orders\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2.4 Create a new column days_since_first_order indicating the number of days since the customer first ordered\u001b[39;00m\n\u001b[32m      9\u001b[39m window_spec = Window.partitionBy(\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m t2DF = \u001b[43mjoined_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal_spend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morder_amount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_spec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m            \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfirst_order_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morder_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_spec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     12\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mdays_since_first_order\u001b[39m\u001b[33m\"\u001b[39m, datediff(current_date(), (col(\u001b[33m\"\u001b[39m\u001b[33morder_date\u001b[39m\u001b[33m\"\u001b[39m))))\n\u001b[32m     14\u001b[39m t2DF.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VajraDev/engineering-learning-journal/.venv/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:1647\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   1643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1644\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1645\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1646\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VajraDev/engineering-learning-journal/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VajraDev/engineering-learning-journal/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: It is not allowed to use a window function inside an aggregate function. Please use the inner window function in a sub-query."
     ]
    }
   ],
   "source": [
    "# transformations \n",
    "# 2.1 Merge the datasets on customer_id\n",
    "joined_df = orders_df.join(customers_df, \"customer_id\")\n",
    "\n",
    "# 2.2 Make all columns uppercase (optional)\n",
    "\n",
    "# 2.3 Calculate each customer's total spend across all orders\n",
    "# 2.4 Create a new column days_since_first_order indicating the number of days since the customer first ordered\n",
    "window_spec = Window.partitionBy(\"customer_id\")\n",
    "t2DF = joined_df.withColumn(\"total_spend\", sum(col(\"order_amount\")).over(window_spec)) \\\n",
    "            .withColumn(\"first_order_date\", min(col(\"order_date\").over(window_spec))) \\\n",
    "    .withColumn(\"days_since_first_order\", datediff(current_date(), (col(\"order_date\"))))\n",
    "\n",
    "t2DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36d310-7b97-4db5-a406-c3cc4ef250a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning Journal (PySpark)",
   "language": "python",
   "name": "learning-journal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
