{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c772e4-d78e-4b3f-b5fe-7d954711d431",
   "metadata": {},
   "source": [
    "### PySpark Data Processing Project Checklist\n",
    "\n",
    "#### 1. Data Loading\n",
    "- [ ] Load `orders.csv` into a Spark DataFrame\n",
    "- [ ] Load `customers.csv` into a Spark DataFrame\n",
    "\n",
    "#### 2. Data Transformation\n",
    "- [ ] Merge the datasets on `customer_id`\n",
    "- [ ] Make all columns uppercase \n",
    "- [ ] Calculate each customer's total spend across all orders\n",
    "- [ ] Create a new column `days_since_first_order` indicating the number of days since the customer first ordered\n",
    "\n",
    "#### 3. Aggregation\n",
    "- [ ] Group data by `product_category`\n",
    "- [ ] Calculate the total order amount for each category\n",
    "- [ ] Identify the top 3 customers based on total spend\n",
    "\n",
    "#### 4. Data Export\n",
    "- [ ] Create `customer_spend.csv` with columns:\n",
    "  - [ ] `customer_id`\n",
    "  - [ ] `customer_name`\n",
    "  - [ ] `total_spend`\n",
    "  - [ ] `days_since_first_order`\n",
    "- [ ] Create `category_summary.csv` with columns:\n",
    "  - [ ] `product_category`\n",
    "  - [ ] `total_order_amount`\n",
    "- [ ] Verify both CSV files were saved correctly\n",
    "\n",
    "#### 5. Validation (Optional)\n",
    "- [ ] Verify row counts match expectations\n",
    "- [ ] Check for null values in critical columns\n",
    "- [ ] Validate calculations with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4318c64c-2ab0-4875-9ab6-df869d1a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session \n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sum, avg, count, row_number, round, dayofmonth, min, max, current_date, datediff, upper, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"ecomm_orders_etl\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82108c5e-8653-4e0e-be03-eba5f622e0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning Journal (PySpark)",
   "language": "python",
   "name": "learning-journal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
